<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cloud on Chasing1020</title><link>https://chasing1020.github.io/tags/cloud/</link><description>Recent content in Cloud on Chasing1020</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 03 Jan 2022 15:53:45 +0800</lastBuildDate><atom:link href="https://chasing1020.github.io/tags/cloud/index.xml" rel="self" type="application/rss+xml"/><item><title>AKF Availability Cube</title><link>https://chasing1020.github.io/post/akf-availability-cube/</link><pubDate>Mon, 03 Jan 2022 15:53:45 +0800</pubDate><guid>https://chasing1020.github.io/post/akf-availability-cube/</guid><description>&lt;img src="https://chasing1020.github.io/post/akf-availability-cube/AKF-Cube.webp" alt="Featured image of post AKF Availability Cube" />&lt;h1 id="akf-availability-cube">AKF Availability Cube&lt;/h1>
&lt;p>随着业务规模增长，拆解单体应用（monolith），设计成为面向服务架构（Service-oriented architectutre），做成一个个微服务（Micro-service）已经是如今的大趋势。&lt;/p>
&lt;h2 id="scale-cube-model">Scale Cube Model&lt;/h2>
&lt;p>从拆分的角度来看，也分成多个可以考虑的维度，其中最常用定义分割服务、定义微服务和扩展产品的模型即是比例立方模型。其中保证可用性的解决方案AKF Availability Cube便是最经典的模型。用于指导有关如何实现高可用性的讨论，以及评估现有系统理论上“设计”可用性的工具。它是关于高可用性设计的两部分系列中的第一部分。 &lt;img src="https://akfpartners.com/uploads/blog/Availability_Cube_draft.jpg"
loading="lazy"
>&lt;/p>
&lt;h2 id="measuring-availability">Measuring Availability&lt;/h2>
&lt;p>互联网产品一直在“服务始终可用”的追求上尽可能减少成本，从这个角度出发，服务衡量标准测度为追求可用性改进提供了一个起点。&lt;/p>
&lt;blockquote>
&lt;p>Clock is not the best measure&lt;/p>
&lt;/blockquote>
&lt;p>时间对于业务的影响是不相等的，这取决于业务是否处于高峰期；企业的商业术语：收入、成本、利润、投资回报率，都以金钱而非时间作为衡量；基础设施组件的时间作为测度是不准确的，他们并不会捕获程序的异常，尽管服务无法运行。&lt;/p>
&lt;blockquote>
&lt;p>Transactional Metics&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Rates: 以速度记录，例如登录、添加到购物车、注册、下载、订单等。应用统计过程控制或其他分析方法来建立指示交易速度异常偏差的阈值。&lt;/li>
&lt;li>Ratios: 预料外的或失败的结果的比例可用于衡量服务质量。对此类比率的分析将建立异常的偏差水平。&lt;/li>
&lt;li>Patterns: 交易模式可以识别预期的活动，没有预期的模式更改可能表示您的产品或服务存在可用性问题。&lt;/li>
&lt;/ul>
&lt;h2 id="3-dimensions-of-scaling">3-Dimensions of scaling&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dimension&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>X-Axis&lt;/td>
&lt;td>Horizontal Duplication and Cloning of services and data&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Y-Axis&lt;/td>
&lt;td>Functional Decomposition and Segmentation - Microservices (or micro-services)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Z-Axis&lt;/td>
&lt;td>Service and Data Partitioning along Customer Boundaries - Shards/Pods&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="x-axis-scaling">X-axis scaling&lt;/h2>
&lt;p>X轴的核心思路在于&amp;quot;cloning/replication&amp;quot;，我们将应用复制，以提高可用性。我们假定整体可用率&lt;/p>
&lt;p>$P_{singleton\ availability}=\frac{Total\ Available-Non\ Available}{Total\ Available}$&lt;/p>
&lt;p>当添加了多个结点以后，新的可用性概率评估变为&lt;/p>
&lt;p>$P_{overall\ availability}=1-(1-P_{singleton\ availability})^n$&lt;/p>
&lt;p>但是仅仅做应用复制，也无法解决如下缺点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>大量与会话相关的信息，这些信息通常难以分发或需要持久化到服务器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每个拷贝需要访问所有的数据，对缓存机制要求很高，数据库很可能成为其中的瓶颈。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不会减少日益增长的开发复杂度。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="y-axis-scaling">Y-axis scaling&lt;/h2>
&lt;p>Y轴通过两个角度进行服务切分。分别是动词（操作流程）和名词（对象类型）：&lt;/p>
&lt;p>按动词分割：服务1只负责购买流程，服务2只负责售后流程，服务3只负责广告投放流程&lt;/p>
&lt;p>按名词分割：服务1只负责商品信息；服务2只负责用户信息。&lt;/p>
&lt;p>微服务的设计核心思路便体现在Y轴，其可以将目标应用划分泳道，限制应用的故障域，将应用故障保持在边界内，不会传播影响外界的服务。&lt;/p>
&lt;p>然而，Y轴划分的越细致，越复杂的系统越容易产生“多米诺骨牌”效应，可能导致相邻系统瘫痪，从而导致延迟、停机和/或完全失败。&lt;/p>
&lt;p>比较合理的方式是：将服务集成在一个隔离的堆栈或泳道内（托管云或公共云），以避免跨数据中心调用。并同时在X轴角度，复制服务，以便每个数据中心或云实例都拥有所需的一切。&lt;/p>
&lt;p>综上，Y轴划分的服务需要在&lt;code>增加服务数量提高可用性&lt;/code>与&lt;code>减少服务间通信损失&lt;/code>做出一个Trade off。&lt;/p>
&lt;h2 id="z-axis-scaling">Z-axis scaling&lt;/h2>
&lt;p>Z轴与X轴很像，区别在于Z轴负责的是数据的子集而不是复制，常见于数据库分片。&lt;/p>
&lt;p>对于每一个Z轴的分片，软件本质上是相同的，区别仅仅在于数据，其定义了“爆炸半径”（blast radius, aka Swimlanes or Bulkhead），边界假设在泳道之间不存在同步调用。提高了解决方案的事务可扩展性，同时能够实现故障的隔离。&lt;/p>
&lt;p>缓存命中率通常随着较小的数据集而上升，优化了缓存利用率，减少内存使用和I/O。&lt;/p>
&lt;p>随着可以使用商品服务器或较小的 IaaS 实例，运营成本通常会下降。&lt;/p>
&lt;p>但是另一个角度，Z轴的分片也带来了如下缺点：&lt;/p>
&lt;ul>
&lt;li>增加了整体的复杂度。&lt;/li>
&lt;li>需要实现分片机制，这个机制难以变更，对新的需求不友好。&lt;/li>
&lt;/ul></description></item><item><title>Kubernetes</title><link>https://chasing1020.github.io/post/kubernetes/</link><pubDate>Fri, 29 Oct 2021 15:40:00 +0800</pubDate><guid>https://chasing1020.github.io/post/kubernetes/</guid><description>&lt;img src="https://chasing1020.github.io/post/kubernetes/Kubernetes.webp" alt="Featured image of post Kubernetes" />&lt;h1 id="kubernetes">Kubernetes&lt;/h1>
&lt;p>场景：管理容器化的工作负载和服务，可促进声明式配置和自动化&lt;/p>
&lt;p>功能：&lt;strong>服务发现和负载均衡&lt;/strong>、&lt;strong>存储编排&lt;/strong>、&lt;strong>自动部署和回滚&lt;/strong>、&lt;strong>自动完成装箱计算&lt;/strong>、&lt;strong>自我修复&lt;/strong>、&lt;strong>密钥与配置管理&lt;/strong>&lt;/p>
&lt;h2 id="1-component">1. Component&lt;/h2>
&lt;p>一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。&lt;/p>
&lt;p>工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。&lt;/p>
&lt;p>&lt;img src="https://d33wubrfki0l68.cloudfront.net/2475489eaf20163ec0f54ddc1d92aa8d4c87c96b/e7c81/images/docs/components-of-kubernetes.svg"
loading="lazy"
alt="Kubernetes 组件"
>&lt;/p>
&lt;h3 id="11-control-plane-components">1.1. Control Plane Components&lt;/h3>
&lt;p>控制面板用于控制整个集群的工作，其包含多个组件，可以运行在单个主机上或者通过副本分别不是在多个主节点用以确保高可用&lt;/p>
&lt;h4 id="kube-apiserver">kube-apiserver&lt;/h4>
&lt;p>API 服务器是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。&lt;/p>
&lt;h4 id="etcd">etcd&lt;/h4>
&lt;p>etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。&lt;/p>
&lt;h4 id="kube-scheduler">kube-scheduler&lt;/h4>
&lt;p>控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。&lt;/p>
&lt;h4 id="kube-controller-manager">kube-controller-manager&lt;/h4>
&lt;p>从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。&lt;/p>
&lt;ul>
&lt;li>节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应&lt;/li>
&lt;li>任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成&lt;/li>
&lt;li>端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)&lt;/li>
&lt;li>服务帐户和令牌控制器（Service Account &amp;amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌&lt;/li>
&lt;/ul>
&lt;h4 id="cloud-controller-manager">cloud-controller-manager&lt;/h4>
&lt;p>云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。
cloud-controller-manager 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。&lt;/p>
&lt;h3 id="12-node-component">1.2. Node Component&lt;/h3>
&lt;h4 id="kubelet">kubelet&lt;/h4>
&lt;p>一个在集群中每个&lt;a class="link" href="https://kubernetes.io/zh/docs/concepts/architecture/nodes/" target="_blank" rel="noopener"
>节点（node）&lt;/a>上运行的代理。 它保证&lt;a class="link" href="https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/#why-containers" target="_blank" rel="noopener"
>容器（containers）&lt;/a>都 运行在 &lt;a class="link" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/" target="_blank" rel="noopener"
>Pod&lt;/a> 中。&lt;/p>
&lt;p>kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。&lt;/p>
&lt;h4 id="kube-proxy">kube-proxy&lt;/h4>
&lt;p>&lt;a class="link" href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" rel="noopener"
>kube-proxy&lt;/a> 是集群中每个节点上运行的网络代理， 实现 Kubernetes &lt;a class="link" href="https://kubernetes.io/zh/docs/concepts/services-networking/service/" target="_blank" rel="noopener"
>服务（Service）&lt;/a> 概念的一部分，负责组件之间的负载均衡网络流量。&lt;/p>
&lt;p>kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。&lt;/p>
&lt;h4 id="container-runtime">Container Runtime&lt;/h4>
&lt;p>容器运行环境是负责运行容器的软件。&lt;/p>
&lt;p>Kubernetes 支持多个容器运行环境: &lt;a class="link" href="https://kubernetes.io/zh/docs/reference/kubectl/docker-cli-to-kubectl/" target="_blank" rel="noopener"
>Docker&lt;/a>、 &lt;a class="link" href="https://containerd.io/docs/" target="_blank" rel="noopener"
>containerd&lt;/a>、&lt;a class="link" href="https://cri-o.io/#what-is-cri-o" target="_blank" rel="noopener"
>CRI-O&lt;/a> 以及任何实现 &lt;a class="link" href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md" target="_blank" rel="noopener"
>Kubernetes CRI (容器运行环境接口)&lt;/a>。&lt;/p>
&lt;h3 id="13-objects">1.3. Objects&lt;/h3>
&lt;p>在 Kubernetes 系统中，&lt;em>Kubernetes 对象&lt;/em> 是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息：&lt;/p>
&lt;ul>
&lt;li>哪些容器化应用在运行（以及在哪些节点上）&lt;/li>
&lt;li>可以被应用使用的资源&lt;/li>
&lt;li>关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略&lt;/li>
&lt;/ul>
&lt;h2 id="2-minikube-quick-start">2. Minikube Quick Start&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sudo install minikube-linux-amd64 /usr/local/bin/minikube
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">minikube start
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">minikube dashboard
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在浏览器出现minikube仪表盘即安装成功。&lt;/p>
&lt;p>使用 &lt;code>kubectl create&lt;/code> 命令创建管理 Pod 的 Deployment。该 Pod 根据提供的 Docker 镜像运行 Container。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">kubectl create deployment hello-node --image&lt;span class="o">=&lt;/span>k8s.gcr.io/echoserver:1.4
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>查看 Deployment：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">kubectl get deployments
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>查看 Pod：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">kubectl get pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出结果类似于这样：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hello-node-5f76cf6ccf-br9b5 1/1 Running 0 1m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>每当运行kubectl时，即像RESTAPI服务器发送请求，创建Pod并且调度到工作节点，kubetlet收到通知，再去告诉Docker运行镜像&lt;/p>
&lt;h2 id="3-pod">3. Pod&lt;/h2>
&lt;h3 id="31-basic">3.1. Basic&lt;/h3>
&lt;p>Pod作为一组并置的容器，代表了k8s中的基本构建模型，一个Pod的所有容器只能运行在同一个节点上。&lt;/p>
&lt;p>无论是IPC还是本地文件通信，在一个独立机器上显得十分合理。&lt;/p>
&lt;p>同一个Pod下的所有容器共享Linux的namespace与network interface，因此可以通过IPC进行通信。同一个Pod下的端口号会发生冲突，但是一个Pod中的所有容器也有相同的loopback网络接口，即可以使用localhost与其他容器进行通信。在Pod之间都在一个共享的网络地址空间中，即不需要经过NAT转换。&lt;/p>
&lt;p>Pod常用的命令&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl explain pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubetcl create -f xx.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl get po xx -o json
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete po xx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl logs xx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl port-forward xx 8888:8080
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubetcl get ns
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create namespace mynamespace
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>通过yaml定义的Pod，包含metadata：名称命名空间，标签；spec：Pod实际说明，包括容器，数据卷等；status：Pod当前信息，所处条件，容器状态。&lt;/p>
&lt;p>此外，还可以手动给Pod添加标签，每一个Pod有两种标签，在yaml的metadata处为Pod附加标签&lt;/p>
&lt;p>app：指定app属于哪个应用、组建或者微服务&lt;/p>
&lt;p>rel：显示Pod运行应用程序版本stable、beta、canary&lt;/p>
&lt;h3 id="32-workloads-controller">3.2. Workloads Controller&lt;/h3>
&lt;h4 id="321-liveness-probe">3.2.1 Liveness Probe&lt;/h4>
&lt;p>通过存活探针liveness probe检测容器是否正在运行，常用方式有：HTTP GET、TCP socket、Exec command。&lt;/p>
&lt;p>在spec.containers下面定义livenessProbe并设置path&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl describe po kubia-liveness
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="322-replicatoincontroller">3.2.2. ReplicatoinController&lt;/h4>
&lt;p>ReplicationController用于确保pod始终是可以运行状态，如果Pod以任何原因消失，那么ReplicationController会注意到缺少，并创建一个Pod替代。运行时，Replication会持续监控正在运行的Pod列表，保证相对应的类型的Pod与预期相符，就算有多余Pod也会删除。&lt;/p>
&lt;p>可以确保一个（或多个）Pod持续运行，方法是在现有Pod丢失时启动新的Pod。&lt;/p>
&lt;p>通过创建kind: ReplicationController来定义，spec.replicas: x决定目标数目，template下定义创建新的Pod所用的模版。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl get rc
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl describe rc kubia
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl label pod xx &lt;span class="nv">app&lt;/span>&lt;span class="o">=&lt;/span>foo --overwrite &lt;span class="c1"># 修改后将不再由该RC管理&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl edit rc kubia
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 当然，可以使用export KUBE_EDITOR=&amp;#34;/usr/bin/vim&amp;#34;来决定编辑器&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl scale rc kubia --replicas&lt;span class="o">=&lt;/span>&lt;span class="m">10&lt;/span> &lt;span class="c1"># 扩容&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其主要由三个部分构成：&lt;/p>
&lt;p>label selector：确定当前ReplicationController作用域下有多少Pod&lt;/p>
&lt;p>replica count：副本个数，指定运行的数量&lt;/p>
&lt;p>pod template：用于创建新的pod副本&lt;/p>
&lt;h4 id="323-replicaset">3.2.3. ReplicaSet&lt;/h4>
&lt;p>相较于ReplicationController，ReplicaSet具有类似的行为，但是可以更好地表达Pod选择器，可以允许匹配缺少某个标签的Pod，或者包含特定标签名的Pod。&lt;/p>
&lt;p>其包含四个特定字段：&lt;/p>
&lt;p>In：label的值必须与其中一个指定的values匹配&lt;/p>
&lt;p>NotIn：Label的值与任何一个values不匹配。&lt;/p>
&lt;p>Exists：Pod必须包含一个指定名称的标签，使用此运算符不应该包括values字段。&lt;/p>
&lt;p>DosNotExist：Pod不得包含有指定名称的标签，同样不包括values字段。&lt;/p>
&lt;h4 id="324-daemonset">3.2.4. DaemonSet&lt;/h4>
&lt;p>&lt;em>DaemonSet&lt;/em> 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。&lt;/p>
&lt;p>DaemonSet 的一些典型用法：&lt;/p>
&lt;ul>
&lt;li>在每个节点上运行集群守护进程&lt;/li>
&lt;li>在每个节点上运行日志收集守护进程&lt;/li>
&lt;li>在每个节点上运行监控守护进程&lt;/li>
&lt;/ul>
&lt;p>通过给Node打上标签&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl label node xx &lt;span class="nv">disk&lt;/span>&lt;span class="o">=&lt;/span>ssd
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>即可以对该节点保证运行设定好的校验ssd的Pod。&lt;/p>
&lt;h4 id="325-jobs">3.2.5. Jobs&lt;/h4>
&lt;p>Job 会创建一个或者多个 Pods，并将继续重试 Pods 的执行，直到指定数量的 Pods 成功终止。 随着 Pods 成功结束，Job 跟踪记录成功完成的 Pods 个数。 当数量达到指定的成功个数阈值时，任务（即 Job）结束。 删除 Job 的操作会清除所创建的全部 Pods。 挂起 Job 的操作会删除 Job 的所有活跃 Pod，直到 Job 被再次恢复执行。&lt;/p>
&lt;p>一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。 当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job 对象会启动一个新的 Pod。&lt;/p>
&lt;p>你也可以使用 Job 以并行的方式运行多个 Pod。&lt;/p>
&lt;p>在Job中可以定义运行多个Pod示例，包括顺序，并行，CronJob等&lt;/p>
&lt;h2 id="4-service">4. Service&lt;/h2>
&lt;p>由于Pod的可变动性，导致其随时会被启动或者关闭，并且在Pod启动钱，会给已经调度到节点上的Pod分配IP地址，且由于水平伸缩的特性，多个Pod可能会提供相同的服务。&lt;/p>
&lt;p>通过创建yml文件定义service&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl get svc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以通过创建服务，来让一个单一稳定的IP访问到Pod，在服务的整个生命周期内，这个地址会保持不变，在服务后的Pod可以删除重建，但是对外的IP不会变化。&lt;/p>
&lt;p>为了外部连接k8s的服务，在服务和pod之间不会直接相连的，而是存在一个endpoint资源，用以暴露服务ip和端口的列表。&lt;/p>
&lt;p>将服务暴露给客户端，有如下几种方式：&lt;/p>
&lt;ul>
&lt;li>NodePort：每个集群节点都会打开一个端口，并将在该端口上接受到的流量重定向到基础服务。&lt;/li>
&lt;li>LoadBalance：使得服务可以通过一个专用的负载均衡器进行访问，&lt;/li>
&lt;li>Ingress：通过一个IP地址公开多个服务&lt;/li>
&lt;/ul></description></item></channel></rss>